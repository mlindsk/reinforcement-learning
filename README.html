<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2023-09-13 Wed 08:47 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="author" content="Mads Lindskou" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<style>*{font-size: x-large;}</style>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org4f0055b">1. 1) Welcome</a>
<ul>
<li><a href="#org5954c84">1.1. RL Approaches:</a></li>
</ul>
</li>
<li><a href="#org4e672ab">2. 2) Return of the Multi-Armed Bandit (explore vs exploit)</a>
<ul>
<li><a href="#org13b72e7">2.1. Epsilon-Greedy</a></li>
<li><a href="#org88d7899">2.2. Optimistic Initial Values</a></li>
<li><a href="#org883872c">2.3. UCB1 (Upper Confidence Bound)</a></li>
<li><a href="#org33386d9">2.4. Thompson Sampling</a></li>
<li><a href="#org3f7ffec">2.5. Thompson Sampling wit Real-Valued (Gaussian) Rewards</a></li>
<li><a href="#org62b5399">2.6. Non-stationary Rewards</a></li>
</ul>
</li>
<li><a href="#orgedfd73a">3. MDPs (model-based approach)</a>
<ul>
<li>
<ul>
<li><a href="#orgd838d1d">3.0.1. Reward function (discounted):</a></li>
<li><a href="#org0901230">3.0.2. Policy functions</a></li>
<li><a href="#org15fd8b7">3.0.3. State-value function</a></li>
<li><a href="#orga32bfa2">3.0.4. Bellman Equation</a></li>
<li><a href="#org157aaa7">3.0.5. Action-value function</a></li>
<li><a href="#orgcea4af8">3.0.6. Bellman Examples</a></li>
<li><a href="#org0fc270e">3.0.7. Optimal Policy and Value Function</a></li>
<li><a href="#orga908531">3.0.8. Theorem: Bellman optimality equation</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org45eac5d">4. Dynamic Programming (DP)</a>
<ul>
<li><a href="#org3df93f4">4.1. Recap</a></li>
<li><a href="#org2978da6">4.2. Iterative Policy Evaluation</a></li>
<li><a href="#orgaf6ed3a">4.3. Policy Improvement</a></li>
<li><a href="#org4d25eb9">4.4. Policy Iteration</a></li>
<li><a href="#orge0b931f">4.5. Value Iteration</a></li>
<li><a href="#orga505ecc">4.6. Comparison of Policy Iteration and Value Iteration</a></li>
</ul>
</li>
<li><a href="#org1d56abe">5. Monte Carlo (model-free approach)</a>
<ul>
<li><a href="#orgd30adc9">5.1. Monte Carlo First Visit (Evaluation)</a></li>
<li><a href="#orga3279fc">5.2. Monte Carlo Exploring Starts (Control)</a></li>
<li><a href="#orgb98f58e">5.3. Epsilon-Greedy Monte Carlo (Control)</a></li>
</ul>
</li>
<li><a href="#org51c8687">6. On-Policy vs Off-Policy</a></li>
<li><a href="#org8ffb491">7. RL and Data</a></li>
</ul>
</div>
</div>
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=0Kw-VTym9Pg">https://www.youtube.com/watch?v=0Kw-VTym9Pg</a></li>
<li></li>
</ul>

<div id="outline-container-org4f0055b" class="outline-2">
<h2 id="org4f0055b"><span class="section-number-2">1.</span> 1) Welcome</h2>
<div class="outline-text-2" id="text-1">
<p>
These are my personal notes on reinforcement learning which is mostly based on the udemy course
</p>

<p>
<a href="https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/">https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/</a>
</p>


<p>
Link to code: <a href="https://github.com/lazyprogrammer/machine_learning_examples">https://github.com/lazyprogrammer/machine_learning_examples</a>
</p>
</div>

<div id="outline-container-org5954c84" class="outline-3">
<h3 id="org5954c84"><span class="section-number-3">1.1.</span> RL Approaches:</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>Dynamic programming (very restrictive and hence more robust)</li>
<li>Monte carlo (lesser restrictive)</li>
<li>TDL includes Q-learning (least restrictive)</li>
</ul>


<div id="org68a6e41" class="figure">
<p><img src="img/1-rl-overview.png" alt="1-rl-overview.png" width="500" style="border:2px solid black;" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org4e672ab" class="outline-2">
<h2 id="org4e672ab"><span class="section-number-2">2.</span> 2) Return of the Multi-Armed Bandit (explore vs exploit)</h2>
<div class="outline-text-2" id="text-2">
<p>
Major application of bandits is to COMPARE items/elements (online advertising, websites, &#x2026;)
</p>

<p>
The <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">multi-armed bandit</a> problem models an agent that
simultaneously attempts to acquire new knowledge (called "exploration") and optimize their decisions based on
existing knowledge (called "exploitation"). The agent attempts to balance these competing tasks in order to maximize
their total value over the period of time considered. 
</p>
</div>

<div id="outline-container-org13b72e7" class="outline-3">
<h3 id="org13b72e7"><span class="section-number-3">2.1.</span> Epsilon-Greedy</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Modifying the 'greedy' strategy (ML/argmax in connection to two bandits). Thus, controlling \(p\) lets us <b><b>explore</b></b>
(to collect data about each bandit) instead og <b><b>exploit</b></b>. Epsilon greedy basically ensures
that we don't get stuck in some 'bad maxmimum likelood estimate'.
</p>


<div id="orga9e22ff" class="figure">
<p><img src="img/2-epsilon-greedy.png" alt="2-epsilon-greedy.png" width="500" style="border:2px solid black;" />
</p>
</div>

<p>
SB pseudo code is given as:
</p>


<div id="orgfe69225" class="figure">
<p><img src="img/2-bs-epsilon-greedy-pseudo-alg.png" alt="2-bs-epsilon-greedy-pseudo-alg.png" />
</p>
</div>

<p>
Here \(Q(a)\) is the expected reward untill now and \(N(a)\) the number of times
we chose action \(a\).
</p>

<p>
Given two bandits with 90% and 80% winrate respectively, the expected reward is:
</p>


<div id="org075e77d" class="figure">
<p><img src="img/2-multi-armed-bandit-reward.png" alt="2-multi-armed-bandit-reward.png" width="500" style="border:2px solid black;" />
</p>
</div>

<p>
So no matter how small epsilon is, we can never expect to go beyond 90% winrate. To
remedy this we can use one of the decaying functions:
</p>


<div id="org6ec42d8" class="figure">
<p><img src="img/2-multi-armed-bandit-epsilon-functions.png" alt="2-multi-armed-bandit-epsilon-functions.png" width="500" style="border:2px solid black;" />
</p>
</div>
</div>
</div>


<div id="outline-container-org88d7899" class="outline-3">
<h3 id="org88d7899"><span class="section-number-3">2.2.</span> Optimistic Initial Values</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Overestimate the true mean (instead of setting it to zero) in the greedy algorithm.
Note here, it is not the epsilon-greedy algorithm.
</p>

<p>
This will cause the algorithm to explore more in the beginning because it will (falsely) believe
that the bandits have high expected rewards. Setting the initial value (hyperparameter)
essential controls the ratio of exploration.
</p>
</div>
</div>


<div id="outline-container-org883872c" class="outline-3">
<h3 id="org883872c"><span class="section-number-3">2.3.</span> UCB1 (Upper Confidence Bound)</h3>
<div class="outline-text-3" id="text-2-3">
<p>
There are several inequalities that state something about the sample mean. In UCB1 we
use Hoefding's inequality (the proof of this inequality is actually a lot of fun!):
</p>


<div id="orgf448d9c" class="figure">
<p><img src="img/2-hoefding.png" alt="2-hoefding.png" width="500" style="border:2px solid black;" />
</p>
</div>

<p>
From this identity the UCB1 update can be derived as
</p>


<div id="orgb2eb986" class="figure">
<p><img src="img/2-ucb1-update.png" alt="2-ucb1-update.png" width="500" style="border:2px solid black;" />
</p>
</div>

<p>
The "2" is a hyperparameter. Here \(n_j\) is the number of times bandit \(j\) has been chosen
and \(N\) is the total number of times we played any bandit. And just to be clear, \(X_{n_j}\)
is the reward (zero or one) when pulling the \(j\)'th bandit (hence \(\bar X_{n_j}\) is the expected reward)
which is a number between zero and one (notice, it is not the cummulative expected reward, but the expected
reward in each pull).
</p>

<p>
Ignoring bandit \(j\) for a long time, means that the square root part will start increasing,
and therefore we start slowly to explore \(j\) again (but only slowly).
</p>
</div>
</div>


<div id="outline-container-org33386d9" class="outline-3">
<h3 id="org33386d9"><span class="section-number-3">2.4.</span> Thompson Sampling</h3>
<div class="outline-text-3" id="text-2-4">
<p>
<a href="https://en.wikipedia.org/wiki/Thompson_sampling#:~:text=Thompson%20sampling%2C%20named%20after%20William,to%20a%20randomly%20drawn%20belief">https://en.wikipedia.org/wiki/Thompson_sampling#:~:text=Thompson%20sampling%2C%20named%20after%20William,to%20a%20randomly%20drawn%20belief</a>.
</p>

<p>
Thompson sampling uses Bayesian conjugate priors (beta is conjugate prior for bernoulli) to update the belief
of the probabilities. This is very well explained in the image below. Notice, that we don't rely on
any 'collected' data, but we update as information is coming in.
</p>


<div id="orge54aa15" class="figure">
<p><img src="img/2-beta-posterior.png" alt="2-beta-posterior.png" width="500" style="border:2px solid black;" />
</p>
</div>


<div id="org9187f26" class="figure">
<p><img src="img/2-beta-update.png" alt="2-beta-update.png" width="500" style="border:2px solid black;" />
</p>
</div>

<p>
We just need to pick some initial values of \(\alpha\) and \(\beta\). We can even encode prior knowledge
into this prior using \(\alpha\) and \(\beta\). Common choices are \(\alpha=1\) and \(\beta=1\) which
leaves us with a uniform distribution on \([0; 1]\)
</p>

<p>
Now instead of using the upper bound in UCB we draw a sample (which is between zero and one)
from the posterior, which is called Thompson sampling and update (alpha and bete) the posterior distribution. We
choose to pull the bandit for which we drew the highest number. The optimal bandit will become skinny in the end.
</p>
</div>
</div>

<div id="outline-container-org3f7ffec" class="outline-3">
<h3 id="org3f7ffec"><span class="section-number-3">2.5.</span> Thompson Sampling wit Real-Valued (Gaussian) Rewards</h3>
<div class="outline-text-3" id="text-2-5">
<p>
Just use the same algorithm, but update according to a Gaussian posterior. Instead of choosing the bandit
with the largest probability of success, we choose the bandit which yields the greates expected reward.
</p>
</div>
</div>


<div id="outline-container-org62b5399" class="outline-3">
<h3 id="org62b5399"><span class="section-number-3">2.6.</span> Non-stationary Rewards</h3>
<div class="outline-text-3" id="text-2-6">
<p>
The above examples relied on i.i.d assumption. Instead of updating the mean as before, we can
update using exponential weighted moving average (EWMA)
</p>


<div id="org95d057d" class="figure">
<p><img src="img/2-ewma.png" alt="2-ewma.png" width="500" style="border:2px solid black;" />
</p>
</div>


<p>
"The older the data is, the less it should contribute to the overall mean". The above equation
can be written as an infinite power series with the term \((1-\alpha)^N\) (the exponential series).
</p>
</div>
</div>
</div>


<div id="outline-container-orgedfd73a" class="outline-2">
<h2 id="orgedfd73a"><span class="section-number-2">3.</span> MDPs (model-based approach)</h2>
<div class="outline-text-2" id="text-3">

<div id="orgc8f1378" class="figure">
<p><img src="img/4-highlevel-mdps.png" alt="4-highlevel-mdps.png" width="500" style="border:2px solid black;" />
</p>
</div>


<div id="org25724ac" class="figure">
<p><img src="img/4-mdps.png" alt="4-mdps.png" width="500" style="border:2px solid black;" />
</p>
</div>

<p>
The MDP is governed by the following probability distribution:
</p>


<div id="org8468f62" class="figure">
<p><img src="img/4-mdp-def1.png" alt="4-mdp-def1.png" width="800" style="border:2px solid black;" />
</p>
</div>



<div id="org42f3947" class="figure">
<p><img src="img/4-mdp-def2.png" alt="4-mdp-def2.png" width="800" style="border:2px solid black;" />
</p>
</div>


<div id="org331455c" class="figure">
<p><img src="img/4-reward-hypothesis.png" alt="4-reward-hypothesis.png" width="800" style="border:2px solid black;" />
</p>
</div>
</div>


<div id="outline-container-orgd838d1d" class="outline-4">
<h4 id="orgd838d1d"><span class="section-number-4">3.0.1.</span> Reward function (discounted):</h4>
<div class="outline-text-4" id="text-3-0-1">

<div id="org0abe044" class="figure">
<p><img src="img/4-discounted-return.png" alt="4-discounted-return.png" width="800" style="border:2px solid black;" />
</p>
</div>

<p>
Discounting intuition: The immediate reward is more worth than the reward on a long distant future. It is a hyperparameter that is usually close to one (0.99, 0.98, &#x2026;). Without discounting, the cumulative future rewards could be infinite, and how should one choose between policies that both yield inifinte returns.
</p>


<p>
The reward function can be written recursively which is useful for the later theory and algorithms.
</p>

<div id="orgc80dd0e" class="figure">
<p><img src="img/4-reward-recursion.png" alt="4-reward-recursion.png" width="800" style="border:2px solid black;" />
</p>
</div>
</div>
</div>

<div id="outline-container-org0901230" class="outline-4">
<h4 id="org0901230"><span class="section-number-4">3.0.2.</span> Policy functions</h4>
</div>

<div id="outline-container-org15fd8b7" class="outline-4">
<h4 id="org15fd8b7"><span class="section-number-4">3.0.3.</span> State-value function</h4>
<div class="outline-text-4" id="text-3-0-3">
<p>
The reward function is dependend on the policy, since a policy obviously can yield very different returns. And surely
it is also dependent on the state we are currently in (think of a simple grid world example where we start just
next to the goal. Then the reward can be high. But starting way back, and accumulating negative ones can give
much worse accumulated reward). But the reward can in fact change (as just discussed) during the process/game, so
how can we enven optimize for this seemingly stochastic number -&gt; value functions. We want to maximize the expected
cumulative reward. The value function is given by
</p>


<div id="org759b33c" class="figure">
<p><img src="img/4-value-function.png" alt="4-value-function.png" width="800" style="border:2px solid black;" />
</p>
</div>

<p>
So the value function describes the expected sum of future rewards given that we are in state \(s\) and we follow the policy \(\pi\) from here on.
</p>

<p>
<b>ALMOST ALL OF THE EFFORT IN RL IS DEVOTED TO SOLVE FOR THE VALUE FUNCTION!!!</b>
</p>
</div>
</div>

<div id="outline-container-orga32bfa2" class="outline-4">
<h4 id="orga32bfa2"><span class="section-number-4">3.0.4.</span> Bellman Equation</h4>
<div class="outline-text-4" id="text-3-0-4">
<p>
In order to calculate the value function, we only need to look one step
ahead which is important! Decreases the statspace to search over drastically
in each step (instead of an entire tree of states). BS furthermore use the
law of total expectation to write the Bellman equation as. Remember that
\(\pi(a|s) = p(a|s)\) and use total law of expectation to put in \(A_t\) in
order to exploit the definitions above.
</p>


<div id="org3309e1e" class="figure">
<p><img src="img/4-bellman-equation-bs.jpg" alt="4-bellman-equation-bs.jpg" width="800" style="border:2px solid black;" />
</p>
</div>


<p>
Assuming all the probabilites are known (they are just numbers), then it should be apparent, that this is just a system of linear equations.
Assume there are \(|S| = k\) states, then we have \(k\) equations in \(k\) unknowns. This is ofcourse non-feasible to solve in realworld applications, where the statespace is extremely large.
</p>
</div>
</div>


<div id="outline-container-org157aaa7" class="outline-4">
<h4 id="org157aaa7"><span class="section-number-4">3.0.5.</span> Action-value function</h4>
<div class="outline-text-4" id="text-3-0-5">
<p>
As opposed to the state-value function, which can be used to test a given policy given a particular state, the action-value function can be used to test how good a particular action is in termes of the expected commulative future rewards given a certain state:
</p>


<div id="orgec5328f" class="figure">
<p><img src="img/4-action-value-function2.png" alt="4-action-value-function2.png" width="800" style="border:2px solid black;" />
</p>
</div>

<p>
\(V(s)\) is useful for <span class="underline">evaluating</span> a policy; given a policy, what is the return we can expect.
</p>

<p>
\(Q(s, a)\) is useful for <span class="underline">control</span>; I'm in state \(s\), what is the best action I can take. That is,
compare \(Q(s, a_1)\) and \(Q(s, a_2)\) etc.
</p>

<p>
Hence we can talk about <b><b>Q-table</b></b> which are 2-dim arrays with values of Q given a state and an action. For a given state we can then compare \(Q(s, a_1)\) and \(Q(s, a_2)\).
</p>

<p>
The relations are given by
</p>


<div id="org6d98fbe" class="figure">
<p><img src="img/4-action-value-function-relation.png" alt="4-action-value-function-relation.png" width="600" style="border:2px solid black;" />
</p>
</div>
</div>
</div>


<div id="outline-container-orgcea4af8" class="outline-4">
<h4 id="orgcea4af8"><span class="section-number-4">3.0.6.</span> Bellman Examples</h4>
<div class="outline-text-4" id="text-3-0-6">
<p>
It is important to note, that the value at the final step (when we arrive at at target), is zero,
since the expected future rewards is exactly zero because we are finished. Generally, in toy
examples we can work 'backwards' and plug-in. Without loops this is easy, but with loops,
we need to set up a system of linear equations and solve.
</p>

<p>
Simple examples can be made by constructing simple DAGS with terminal nodes (nodes that have no parents),
and assigning weight probabilities and rewards on each node. In general, note that the reward
is dependent on the previous state and the action taken. This implies, that transitioning to a state
could lead to different rewards based on what the action was to get there!
</p>
</div>
</div>

<div id="outline-container-org0fc270e" class="outline-4">
<h4 id="org0fc270e"><span class="section-number-4">3.0.7.</span> Optimal Policy and Value Function</h4>
<div class="outline-text-4" id="text-3-0-7">
<p>
The policy \(\pi\) is <span class="underline">better</span> that \(\pi'\) if its expected return is greater than that of \(\pi'\) for all statesl; \(\pi \geq \pi'\) if and only if \(v_{\pi}(s) \geq v_{\pi'}(s)\) for all \(s\).
</p>

<p>
The optimal state-value function, policy and action-value function is defined as
</p>


<div id="org7a94e3f" class="figure">
<p><img src="img/4-bellman-optimality-defintion.png" alt="4-bellman-optimality-defintion.png" width="600" style="border:2px solid black;" />
</p>
</div>
</div>
</div>

<div id="outline-container-orga908531" class="outline-4">
<h4 id="orga908531"><span class="section-number-4">3.0.8.</span> Theorem: Bellman optimality equation</h4>
<div class="outline-text-4" id="text-3-0-8">
<p>
The optimal value function is unique, but the optimal policy is not! Note the property that V* = max Q* below, which can be seen from the definitions above.
</p>


<div id="orgfea2b0c" class="figure">
<p><img src="img/4-bellman-optimality-equations.jpg" alt="4-bellman-optimality-equations.jpg" width="600" style="border:2px solid black;" />
</p>
</div>

<p>
Finding v* and q* is just a means to and end where we want an optimal policy! The optimal policy can be found as
</p>



<div id="org35e4429" class="figure">
<p><img src="img/4-optimal-policy.png" alt="4-optimal-policy.png" width="600" style="border:2px solid black;" /> 
</p>
</div>

<p>
But normally we dont know the probability involved; imagine huge statespace and images. So
also very hard to estimate. But in dynamic programming we use this relation.
</p>

<p>
For the action value function we have:
</p>


<div id="org6832e4f" class="figure">
<p><img src="img/4-optimal-policy-action-value.png" alt="4-optimal-policy-action-value.png" width="600" style="border:2px solid black;" />
</p>
</div>

<p>
So we can go about taking max in the a-row in the Q-table. We use this relation
in monte carlo and temporal difference learning.
</p>

<p>
For all RL algos we learn, we'll follow the following pattern:
</p>

<ul class="org-ul">
<li>(Task 1) <b>Evaluation/prediction problem</b> (V) - Evaluate a given policy (i.e, what is the value of V given pi*)</li>
<li>(Task 2) <b>Control problem</b> (Q) - Find the best policy</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org45eac5d" class="outline-2">
<h2 id="org45eac5d"><span class="section-number-2">4.</span> Dynamic Programming (DP)</h2>
<div class="outline-text-2" id="text-4">
<p>
DP refers to the methods used for solving RLs when we have complete knowledge.
</p>
</div>

<div id="outline-container-org3df93f4" class="outline-3">
<h3 id="org3df93f4"><span class="section-number-3">4.1.</span> Recap</h3>
<div class="outline-text-3" id="text-4-1">
<p>
At each time step, the agent recieves a state \(S_t\) and a reward \(R_t\), while the environment then recieves an action from the agent \(A_t\).
The objective is to <b>"program the agent"</b> in order to maxmize the expected future return.
</p>

<p>
The policy, \(\pi(a\mid s)\) (can be deterministic) sort of governs the agent while the joint pmf \(p(s', r \mid s, a)\) governs the environment. And therefore,
we need to find <b>"the best pi"</b>.
</p>
</div>
</div>


<div id="outline-container-org2978da6" class="outline-3">
<h3 id="org2978da6"><span class="section-number-3">4.2.</span> Iterative Policy Evaluation</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Here we solve <b>Task 1</b>. In this section, we simply assume that we know \(\pi(a\mid s)\) and \(p(s', r \mid s, a)\) so we can apply Bellmans equation directly. It is not reasonable to know \(p(s', r \mid s, a)\) but we do it anyway.
</p>


<div id="org0a87e76" class="figure">
<p><img src="img/4-bellman-equation-bs.jpg" alt="4-bellman-equation-bs.jpg" width="800" style="border:2px solid black;" />
</p>
</div>

<p>
Since everything is known, we can simply solve this problem using a system of linear equations. But this is not scalable when the statespace is large.
</p>

<p>
Also, DP can't handle the situation when \(p\) is unkown, but it will lead to methods that can! The iterative policy evaluation algorithm is given below where \(v \leftarrow V(s)\) means the "old value" og the value function for that given state. And the max delta is taken over all states. Also, even simple, remember that we must loop over \(a, s'\) and \(r\)!
</p>


<div id="orgc5ed6d8" class="figure">
<p><img src="img/5-iterative-policy-evaluation.png" alt="5-iterative-policy-evaluation.png" width="800" style="border:2px solid black;" />
</p>
</div>

<p>
It should also be noted, that the <b>reward is deterministic for many practical problems</b> and therefore we can alleviate to sum over the rewards and we have \(p(s', r \mid s, a)\) = \(p(s'\mid s, a)\) and \(r \equiv r(s')\) (don't need to depend on the action, since the action led us to state \(s'\)!). So \(r(s)\) could simply be a, deterministic, mapping from \(S\) to \(R\).
</p>
</div>
</div>


<div id="outline-container-orgaf6ed3a" class="outline-3">
<h3 id="orgaf6ed3a"><span class="section-number-3">4.3.</span> Policy Improvement</h3>
<div class="outline-text-3" id="text-4-3">
<p>
Now, given a (deterministic) policy, how can we improve it iteratively. Suppose that vi know the value function \(v_\pi\) from policy iteration.
For a given state \(s\) we want to know whether it makes sense to, deterministically, change \(\pi(s)\) to an action \(a\). We already know
how good it is to follow \(\pi\) from state \(s\) which is just \(v_\pi(s)\) and therefore we can compare it. So what is the value
if we instead take action \(a\) when in state \(s\), and thereafter follow \(\pi\).
</p>

<p>
So assume we are given
</p>

<ul class="org-ul">
<li>some \(\pi(s)\)</li>
<li>we have found \(V_{\pi}(s)\) and \(Q_{\pi}(s,a)\)</li>
<li>we take an action, \(a\), NOT prescribed by the policy for state \(a\neq s\)</li>
<li>but hereafter we follow \(\pi(s)\) again</li>
</ul>

<p>
This is EXACTLY what \(Q_{\pi}(s,a)\) is. Hence, if
\[
Q_{\pi}(s,a) > V_{\pi}(s)
\]
then, the return for that particular <b>episode</b> is better than if we had just followd \(\pi\) the whole time.
</p>

<p>
Pictorally this can be show as:
</p>


<div id="org35442ba" class="figure">
<p><img src="img/4-policy-sequence.png" alt="4-policy-sequence.png" width="800" style="border:2px solid black;" />
</p>
</div>

<p>
So, for each state, the best action is found by:
</p>


<div id="orgbc9acbb" class="figure">
<p><img src="img/4-optimal-policy-action-value.png" alt="4-optimal-policy-action-value.png" width="600" style="border:2px solid black;" />
</p>
</div>

<p>
So what if we change the action \(\pi(s)\) to \(a^{\ast} = \pi'(s)\) and make a new policy \(\pi\)?
</p>

<p>
<b>POLICY IMPOROVEMENT THEOREM</b>:
</p>


<div id="org7f6eaca" class="figure">
<p><img src="img/5-policy-improvement-thm.png" alt="5-policy-improvement-thm.png" />
</p>
</div>

<p>
This theorem extends to stochastic polies using the natural definition
</p>


<div id="orgc6845bb" class="figure">
<p><img src="img/5-policy-improvement-thm-stochastic.png" alt="5-policy-improvement-thm-stochastic.png" width="600" style="border:2px solid black;" />
</p>
</div>



<div id="orge26dc32" class="figure">
<p><img src="img/5-policy-improvement-thm-equality.png" alt="5-policy-improvement-thm-equality.png" width="600" style="border:2px solid black;" />
</p>
</div>

<p>
The above gives us a criterion for when to stop the policy iteration algorithm (introduced below). When the policy no longer improves,
we say that it is <b>stable</b>. Also, it says that we should take the argmax in each state (the Bellman optimality equation), i.e.
</p>


<div id="org44719b6" class="figure">
<p><img src="img/4-optimal-policy.png" alt="4-optimal-policy.png" width="600" style="border:2px solid black;" /> 
</p>
</div>


<div id="org6fad38a" class="figure">
<p><img src="img/5-policy-improvement-thm-proof.png" alt="5-policy-improvement-thm-proof.png" width="600" style="border:2px solid black;" />
</p>
</div>
</div>
</div>


<div id="outline-container-org4d25eb9" class="outline-3">
<h3 id="org4d25eb9"><span class="section-number-3">4.4.</span> Policy Iteration</h3>
<div class="outline-text-3" id="text-4-4">
<p>
The policy iteration algorithm is composed of the policy evaluation together with the policy improvement algorithms so sequentially update the policy.
</p>


<div id="org6e6818d" class="figure">
<p><img src="img/5-policy-iteration-illustration.png" alt="5-policy-iteration-illustration.png" width="600" style="border:2px solid black;" />
</p>
</div>


<p>
The policy iteration pseudo algorithm from Barto and Sutton is given as below. Note that the value function can be initialized with zeroes (or random except for the terminal states which has to be zero).
</p>

<div id="org2923e4d" class="figure">
<p><img src="img/5-policy-iteration-pseudo-alg.png" alt="5-policy-iteration-pseudo-alg.png" width="800" style="border:2px solid black;" />
</p>
</div>

<p>
Note that policy iteration yields a <b>deterministic</b> policy since we take argmaxes in each step! Since optimal policies are not unique, we could end up getting stuck in the loop. This is not an issue in <b>value iteration</b> where we just compute a SINGLE optimal policy (and dont care about all the other optimal policies).
</p>
</div>
</div>

<div id="outline-container-orge0b931f" class="outline-3">
<h3 id="orge0b931f"><span class="section-number-3">4.5.</span> Value Iteration</h3>
<div class="outline-text-3" id="text-4-5">
<p>
Since the optimal policy can be derived from the Bellman optimality equation
</p>

<div id="org79a897b" class="figure">
<p><img src="img/4-optimal-policy.png" alt="4-optimal-policy.png" width="600" style="border:2px solid black;" /> 
</p>
</div>

<p>
(i.e, from an optimal value function) we can avoid some unncessary loops compared to policy iteration.
</p>



<div id="org80c2d42" class="figure">
<p><img src="img/5-value-iteration-pseudo-alg.png" alt="5-value-iteration-pseudo-alg.png" width="800" style="border:2px solid black;" />
</p>
</div>
</div>
</div>


<div id="outline-container-orga505ecc" class="outline-3">
<h3 id="orga505ecc"><span class="section-number-3">4.6.</span> Comparison of Policy Iteration and Value Iteration</h3>
<div class="outline-text-3" id="text-4-6">

<div id="org33994e3" class="figure">
<p><img src="img/5-compare-algs.png" alt="5-compare-algs.png" width="800" style="border:2px solid black;" />
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org1d56abe" class="outline-2">
<h2 id="org1d56abe"><span class="section-number-2">5.</span> Monte Carlo (model-free approach)</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-orgd30adc9" class="outline-3">
<h3 id="orgd30adc9"><span class="section-number-3">5.1.</span> Monte Carlo First Visit (Evaluation)</h3>
<div class="outline-text-3" id="text-5-1">
<p>
Note here e.g. for model-based approaches, one can design a policy. For example, one can design/learn a policy that takes actions when trading. So the actions could be "buy", "sell" and "flat". Given a certain state, we can estimate this policy.
</p>

<p>
In this section, the transition probabilities are unknown, and the agent must learn to navigate the environment to learn these. MC methods can only be used in problems with terminal nodes, i.e. problems that terminate, since we need to compute the expected future reward.
</p>

<p>
In first visit monte carlo, the idea is to play a bunch of episodes and collect samples of returns, \(G\), and then average these. More specifically, we estimate the value function by sample means.
</p>


<div id="org3488024" class="figure">
<p><img src="img/6-first-visit-mc--pseudo-alg.png" alt="6-first-visit-mc--pseudo-alg.png" width="800" style="border:2px solid black;" />
</p>
</div>

<p>
The algorithm can move into an infinite cycle, and we can just terminate the algo. after a predefined number of steps.
</p>
</div>
</div>

<div id="outline-container-orga3279fc" class="outline-3">
<h3 id="orga3279fc"><span class="section-number-3">5.2.</span> Monte Carlo Exploring Starts (Control)</h3>
<div class="outline-text-3" id="text-5-2">
<p>
The MCES, is used for the control problem and leverages the ideas from FVMC. The idea is to, on an episodic basis, approximate both \(Q\) and \(\pi\). Notice how we cant compute \(\pi\) as per the definition, since we do not know the transition probabilities. However, we do know that \(\pi\) is the arg-max of the state value function \(Q\). And we find \(Q\) by an ordinary average over states and actions.
</p>


<div id="org35c272e" class="figure">
<p><img src="img/6-mces--pseudo-alg.png" alt="6-mces--pseudo-alg.png" width="800" style="border:2px solid black;" />
</p>
</div>

<p>
Notice how it is in-efficient to calculate the mean (for \(Q\)) when the number of samples is large. It can be updated as 
\[
Q(s,a) = Q(s, a) + \frac{1}{N(s,a)}\bigg[G - Q(s,a)\bigg]
\].
</p>

<p>
One crucial assumption in the MCES algorithm is the "exploring starts" which is not always realistic. Imagine a self-driving car or a video game. You cant start in any state in the video game (otherwise just choose the best one :). The reason for randomly selecting a state and action in MCES, is to be sure that we eventually visit all state-action pairs so get more diverse episodes. Ultimately, we want all actions to be chosen infinitelty often, because then the convergence results (optimality) holds true.
</p>
</div>
</div>

<div id="outline-container-orgb98f58e" class="outline-3">
<h3 id="orgb98f58e"><span class="section-number-3">5.3.</span> Epsilon-Greedy Monte Carlo (Control)</h3>
<div class="outline-text-3" id="text-5-3">
<p>
In EGMC we use a soft policy, which means that \(\pi(a|s) > 0\) for all \(s\) and \(a\), but gradually shifted towards a deterministic policy. Specifically, the EPGM uses the $&epsilon;$-greedy policy where 
</p>


<div id="org1f93261" class="figure">
<p><img src="img/6-egmc-pseudo-alg.png" alt="6-egmc-pseudo-alg.png" width="800" style="border:2px solid black;" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org51c8687" class="outline-2">
<h2 id="org51c8687"><span class="section-number-2">6.</span> On-Policy vs Off-Policy</h2>
<div class="outline-text-2" id="text-6">
<ol class="org-ol">
<li><b><b>Definition</b></b>:
<ul class="org-ul">
<li><b><b>On-Policy</b></b>: On-policy methods learn the value or policy function from the data generated by the current policy itself. In other words, they use the data collected by the agent while it follows its current policy to update that same policy.</li>
<li><b><b>Off-Policy</b></b>: Off-policy methods, on the other hand, learn the value or policy function from data collected by following a different, potentially older, or even random policy. They can update a policy while using data collected by a different policy.</li>
</ul></li>

<li><b><b>Data Utilization</b></b>:
<ul class="org-ul">
<li><b><b>On-Policy</b></b>: On-policy methods can only use the data they collect while they are following the current policy. This means they can't make use of past experiences collected under a different policy.</li>
<li><b><b>Off-Policy</b></b>: Off-policy methods can use data collected by any policy, making them more data-efficient. This is particularly advantageous in situations where exploration of the environment is costly or risky.</li>
</ul></li>

<li><b><b>Sample Efficiency</b></b>:
<ul class="org-ul">
<li><b><b>On-Policy</b></b>: On-policy methods are often less sample-efficient compared to off-policy methods because they cannot reuse past data effectively. They need to continually gather new data to update the current policy.</li>
<li><b><b>Off-Policy</b></b>: Off-policy methods tend to be more sample-efficient since they can reuse previously collected data. This can lead to faster learning in situations where collecting data is resource-intensive or time-consuming.</li>
</ul></li>

<li><b><b>Exploration vs. Exploitation</b></b>:
<ul class="org-ul">
<li><b><b>On-Policy</b></b>: On-policy methods may have a slight advantage in exploration because they focus on improving the current policy. However, they can get stuck in local optima if exploration is not balanced.</li>
<li><b><b>Off-Policy</b></b>: Off-policy methods often handle exploration and exploitation more flexibly. They can explore using a stochastic policy while still learning from data collected under a deterministic or less exploratory policy.</li>
</ul></li>

<li><b><b>Examples</b></b>:
<ul class="org-ul">
<li><b><b>On-Policy</b></b>: Methods like REINFORCE and A3C (Asynchronous Advantage Actor-Critic) are on-policy algorithms.</li>
<li><b><b>Off-Policy</b></b>: Q-learning, DDPG (Deep Deterministic Policy Gradients), and TD3 (Twin Delayed Deep Deterministic Policy Gradients) are examples of off-policy algorithms.</li>
</ul></li>
</ol>

<p>
In summary, the choice between on-policy and off-policy methods depends on the specific requirements and constraints of the RL problem you are trying to solve. On-policy methods can be simpler to implement and might be preferred when exploration is a key concern, while off-policy methods are often more sample-efficient and can leverage historical data effectively, making them suitable for a wider range of scenarios.
</p>
</div>
</div>


<div id="outline-container-org8ffb491" class="outline-2">
<h2 id="org8ffb491"><span class="section-number-2">7.</span> RL and Data</h2>
<div class="outline-text-2" id="text-7">
<p>
In reinforcement learning (RL), you can use information from a dataset with variables X and Y (forming the states) in several ways, depending on your specific problem and the RL algorithm you're using. Here are some common approaches:
</p>

<ol class="org-ol">
<li><b><b>State Representation:</b></b> The dataset can serve as a source for defining the state representation for your RL problem. You can use the variables X and Y directly as state features or preprocess them to create a more informative state representation. This representation should capture all relevant information about the environment that the RL agent needs to make decisions.</li>

<li><b><b>Initialization:</b></b> You can use the dataset to initialize the RL agent's policy or value function. For example, you can train a supervised learning model (e.g., a neural network) using the dataset to predict actions or state values. Then, you can use this pre-trained model as the starting point for your RL algorithm.</li>

<li><b><b>Reward Design:</b></b> The dataset can help you design a suitable reward function for your RL problem. By analyzing the data, you can identify patterns or conditions that indicate whether an action is good or bad. This information can guide the creation of a reward function that provides meaningful feedback to the RL agent.</li>

<li><b><b>Behavior Cloning:</b></b> If you have access to a dataset of expert demonstrations (X, Y, actions), you can use behavior cloning to train an RL policy. You train a model (e.g., a neural network) to predict actions from states (X and Y) based on the expert data. The RL agent can then fine-tune this policy using reinforcement learning techniques.</li>

<li><b><b>Imitation Learning:</b></b> Similar to behavior cloning, you can use the dataset for imitation learning, where the RL agent learns from expert demonstrations. Imitation learning methods aim to generalize from the dataset to handle situations not explicitly seen in the data.</li>

<li><b><b>Experience Replay:</b></b> If your RL algorithm supports experience replay (e.g., Deep Q-Networks), you can store the dataset in a replay buffer and sample experiences from it during training. This helps stabilize training by breaking the temporal correlation between consecutive experiences and can make learning more data-efficient.</li>

<li><b><b>Reward Shaping:</b></b> You can analyze the dataset to identify subgoals or intermediate rewards that can be used for reward shaping. Reward shaping can help guide the RL agent toward desired behaviors and accelerate learning.</li>

<li><b><b>Policy Evaluation:</b></b> If you have a dataset of state-action pairs and their associated rewards (X, Y, actions, rewards), you can use it for policy evaluation. This involves estimating the expected return of a policy based on the dataset, which can be useful for comparing different policies or assessing the quality of a learned policy.</li>
</ol>

<p>
The specific approach you choose will depend on the nature of your RL problem and the available dataset. Keep in mind that using a dataset effectively in RL often requires careful preprocessing, data analysis, and consideration of potential challenges such as distribution shift between the dataset and the RL environment. Additionally, it's essential to choose an RL algorithm that supports your chosen approach (e.g., behavior cloning, imitation learning, experience replay) and implement it accordingly.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Mads Lindskou</p>
<p class="date">Created: 2023-09-13 Wed 08:47</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>